# Auto-Persistence Strategy - How It Works

## The Challenge

You want:
1. âœ… Upload data once
2. âœ… Refresh page â†’ data still there
3. âœ… Deploy app â†’ data still there
4. âœ… Everyone sees the same data

## The Solution: Hybrid Approach

### ğŸ“Š How Data Flows

```
User uploads CSV locally
        â†“
Saved to TWO places:
â”œâ”€â”€ data/cache/          (Fast, temporary)
â””â”€â”€ data/sprints/        (Permanent, in Git)
        â†“
You run: ./push_data.sh
        â†“
Git commit + push to GitHub
        â†“
Streamlit Cloud auto-deploys
        â†“
Everyone sees updated data!
```

---

## ğŸ¯ Three-Tier Storage System

### Tier 1: Session State (Immediate)
- **Location:** Memory
- **Lifetime:** Current page session
- **Speed:** Instant
- **Purpose:** While user is actively using app

### Tier 2: Cache (Fast Reload)
- **Location:** `data/cache/` 
- **Lifetime:** Until app restart
- **Speed:** Very fast
- **Purpose:** Survive page refreshes

### Tier 3: Permanent (Git-tracked)
- **Location:** `data/sprints/` and `data/plans/`
- **Lifetime:** Forever (in Git)
- **Speed:** Fast
- **Purpose:** Survive deployments, shared across all users

---

## ğŸ“ File Structure

```
data/
â”œâ”€â”€ cache/                              # Temporary (NOT in Git)
â”‚   â”œâ”€â”€ last_sprint_data.csv           # Latest upload (temp)
â”‚   â”œâ”€â”€ last_plan_data.csv             # Latest plan (temp)
â”‚   â””â”€â”€ metadata.json                  # Upload info
â”‚
â”œâ”€â”€ sprints/                            # Permanent (IN Git)
â”‚   â”œâ”€â”€ sprint_2026-01-23.csv          # Historical sprint
â”‚   â”œâ”€â”€ sprint_2026-02-06.csv          # Another sprint
â”‚   â””â”€â”€ sprint_2026-02-20.csv          # Latest sprint â† Auto-loads this
â”‚
â””â”€â”€ plans/                              # Permanent (IN Git)
    â”œâ”€â”€ plan_2026-01-26.csv
    â””â”€â”€ plan_2026-01-27.csv             # Latest plan â† Auto-loads this
```

---

## ğŸ”„ Complete Workflow

### For You (Data Uploader):

**Step 1: Upload Locally**
```bash
# Run your local app
streamlit run src/app.py

# Upload CSV in browser
# âœ… Data saved to data/cache/ (immediate)
# âœ… Data also saved to data/sprints/ (permanent)
```

**Step 2: Push to Cloud**
```bash
# Sync to GitHub
./push_data.sh

# This does:
# - git add data/
# - git commit
# - git push origin main
```

**Step 3: Streamlit Cloud Auto-Deploys**
- Detects push
- Rebuilds app
- New data is live!

### For Cloud Users (Viewers):

**First Visit:**
1. Open app URL
2. App auto-loads from `data/sprints/sprint_*.csv` (most recent)
3. See latest data immediately!

**After Refresh:**
1. Refresh page
2. App loads from cache (faster!)
3. Data still there

**After You Push Updates:**
1. App restarts automatically
2. Loads new data from Git
3. Everyone sees updates

---

## ğŸš€ Auto-Load Logic

### On App Start:

```python
def auto_save_and_load_wrapper():
    # 1. Check cache first (fastest)
    if cache_exists:
        load_from_cache()
    
    # 2. Otherwise, load from Git-tracked files
    else:
        latest_sprint = get_latest_sprint_file()  # Most recent sprint_*.csv
        latest_plan = get_latest_plan_file()      # Most recent plan_*.csv
        load_and_cache(latest_sprint, latest_plan)
    
    # 3. Show user what was loaded
    display_load_message()
```

### Priority:
1. **Cache** (if exists, use it - fastest)
2. **Git files** (if cache empty, load most recent file)
3. **Nothing** (fresh start, wait for upload)

---

## âœ… Benefits

### For Local Development:
- âœ… Fast reloads (cache)
- âœ… Data persists in files
- âœ… Easy to inspect/edit

### For Cloud Deployment:
- âœ… Data survives app restarts
- âœ… All users see same data
- âœ… Version controlled (Git)
- âœ… No database setup needed

### For Users:
- âœ… No re-uploading after refresh
- âœ… Always see latest data
- âœ… Fast page loads

---

## ğŸ”’ What Gets Committed to Git?

### âœ… Tracked (Committed):
- `data/sprints/sprint_*.csv` 
- `data/plans/plan_*.csv`
- `data/upload_history.csv`
- `data/rules.json`
- All source code

### âŒ NOT Tracked (Excluded):
- `data/cache/*` (temporary)
- `logs/*` (temporary)
- `.venv/` (local Python environment)

---

## ğŸ“ Your Workflow Summary

### Daily Use:

```bash
# Morning: Start local app
streamlit run src/app.py

# Upload new data via browser
# âœ… Auto-saved to cache + permanent storage

# Sync to cloud (when ready)
./push_data.sh

# Done! Cloud users see updates in ~30 seconds
```

### One-Time Setup (Already Done):
- âœ… Git repository initialized
- âœ… Auto-save/load code added
- âœ… Directory structure created
- âœ… .gitignore configured

---

## ğŸ¯ Key Points

1. **You upload locally** (has file access, network access if needed)
2. **Data saved to Git-tracked files** (permanent)
3. **You manually push** when ready (control timing)
4. **Cloud auto-deploys** (users see updates)
5. **Everyone auto-loads** latest data (no manual uploads)

---

## ğŸ’¡ Why This Approach?

### Why Not Auto-Commit from Cloud?
- âŒ Streamlit Cloud is read-only
- âŒ Would need GitHub auth tokens (security risk)
- âŒ Complex setup

### Why Not Database?
- âŒ Costs money
- âŒ More complex
- âŒ CSV files work perfectly fine

### Why Git-Based?
- âœ… Free
- âœ… Version control built-in
- âœ… You already use Git
- âœ… Simple file system
- âœ… Easy to backup/restore

---

## ğŸ”§ Commands Reference

### Push Data to Cloud:
```bash
./push_data.sh
```

### Manual Push:
```bash
git add data/
git commit -m "Update data"
git push origin main
```

### Check What Changed:
```bash
git status
git diff data/
```

### View Cloud Logs:
- Go to Streamlit Cloud dashboard
- Click your app
- View logs tab

---

## ğŸ‰ Result

**Perfect balance of:**
- âœ… Ease of use
- âœ… Data persistence
- âœ… Cost ($0)
- âœ… Your control
- âœ… Auto-loading for everyone

**No complex setup needed - just works!** ğŸš€

---

## Next Steps

1. Test locally (already works!)
2. Push to GitHub: `./push_data.sh`
3. Verify on Streamlit Cloud
4. Users can now refresh without losing data!

Done! The system is ready. ğŸŠ
